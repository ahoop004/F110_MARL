name: gaplock_rainbow_sweep
description: Rainbow DQN hyperparameter sweep - uncomment parameters to add to search

method: bayes
metric:
  name: car_0/rolling/avg_reward
  goal: maximize

project: marl-f110
entity: ahoop004-old-dominion-university

command:
  - ${env}
  - python
  - run_sweep.py
  - --scenario
  - scenarios/v2/gaplock_rainbow.yaml

early_terminate:
  type: hyperband
  min_iter: 300
  max_iter: 1000
  s: 2
  eta: 3

parameters:
  episodes:
    value: 1000

  seed:
    distribution: int_uniform
    min: 0
    max: 999999

  # ============================================================================
  # ACTIVE PARAMETERS - Currently being swept
  # ============================================================================

  # Learning rate (discretized for efficient search)
  agents.car_0.params.lr:
    values: [0.0001, 0.0002, 0.0003, 0.0005]

  # Noisy Networks exploration (discretized)
  agents.car_0.params.noisy_sigma0:
    values: [0.25, 0.35, 0.45]

  # Multi-step returns (1-3 steps)
  agents.car_0.params.n_step:
    values: [1, 2, 3]

  # Target network update frequency (discretized)
  agents.car_0.params.target_update_interval:
    values: [1000, 1500, 2000]

  # Success buffer ratio (critical for preventing forgetting)
  agents.car_0.params.success_buffer_ratio:
    values: [0.3, 0.4, 0.5, 0.6]

  # ============================================================================
  # FIXED PARAMETERS - Using good defaults (uncomment to re-enable sweeping)
  # ============================================================================

  # Distributional RL - number of atoms
  # Standard Rainbow uses 51; higher values add complexity but may not help
  # agents.car_0.params.atoms:
  #   values: [31, 41, 51]

  # Distributional RL - value range minimum
  # Fixed to -150 based on reward range; uncomment to sweep
  # agents.car_0.params.v_min:
  #   distribution: uniform
  #   min: -200.0
  #   max: -100.0

  # Distributional RL - value range maximum
  # Fixed to 250 based on reward range; uncomment to sweep
  # agents.car_0.params.v_max:
  #   distribution: uniform
  #   min: 200.0
  #   max: 300.0

  # PER prioritization exponent
  # Fixed to 0.5 for balanced prioritization; uncomment to sweep
  # agents.car_0.params.per_alpha:
  #   distribution: uniform
  #   min: 0.4
  #   max: 0.6

  # ============================================================================
  # ALTERNATIVE CONFIGURATIONS - Switch between continuous and discrete
  # ============================================================================

  # CONTINUOUS LEARNING RATE (uncomment to use instead of discrete values)
  # agents.car_0.params.lr:
  #   distribution: log_uniform_values
  #   min: 0.0001
  #   max: 0.0005

  # CONTINUOUS NOISY SIGMA (uncomment to use instead of discrete values)
  # agents.car_0.params.noisy_sigma0:
  #   distribution: uniform
  #   min: 0.2
  #   max: 0.5

  # CONTINUOUS TARGET UPDATE INTERVAL (uncomment to use instead of discrete)
  # agents.car_0.params.target_update_interval:
  #   distribution: int_uniform
  #   min: 1000
  #   max: 2500

  # CONTINUOUS SUCCESS BUFFER RATIO (uncomment to use instead of discrete)
  # agents.car_0.params.success_buffer_ratio:
  #   distribution: uniform
  #   min: 0.3
  #   max: 0.6

  # ============================================================================
  # ADDITIONAL PARAMETERS - Advanced tuning (currently disabled)
  # ============================================================================

  # Batch size - affects training stability and speed
  # agents.car_0.params.batch_size:
  #   values: [128, 256, 512]

  # Gamma (discount factor) - how much to value future rewards
  # agents.car_0.params.gamma:
  #   values: [0.99, 0.995, 0.999]

  # Learning starts - minimum buffer size before training
  # agents.car_0.params.learning_starts:
  #   values: [1000, 2500, 5000]

  # PER beta start - importance sampling correction
  # agents.car_0.params.per_beta_start:
  #   distribution: uniform
  #   min: 0.3
  #   max: 0.6

# ============================================================================
# CURRENT CONFIGURATION SUMMARY
# ============================================================================
# Active parameters: 5 (lr, noisy_sigma0, n_step, target_update, success_ratio)
# Search space: 4 × 3 × 3 × 3 × 4 = 432 combinations
# Fixed in scenario: atoms=51, v_min=-150, v_max=250, per_alpha=0.5
# ============================================================================
