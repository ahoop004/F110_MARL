program: run.py
method: bayes
metric:
  name: train/return
  goal: maximize
command:
  - python
  - run.py
  - --scenario
  - scenarios/gaplock_td3.yaml
parameters:
  actor_lr:
    distribution: log_uniform_values
    min: 0.00012
    max: 0.00055
  critic_lr:
    distribution: log_uniform_values
    min: 0.00012
    max: 0.00055
  actor_weight_decay:
    distribution: log_uniform_values
    min: 1e-5
    max: 3e-4
  critic_weight_decay:
    distribution: log_uniform_values
    min: 1e-5
    max: 3e-4
  gamma:
    distribution: uniform
    min: 0.97
    max: 0.999
  tau:
    distribution: log_uniform_values
    min: 0.002
    max: 0.02
  policy_noise:
    distribution: uniform
    min: 0.02
    max: 0.15
  noise_clip:
    distribution: uniform
    min: 0.05
    max: 0.5
  policy_delay:
    values: [1, 2]
  exploration_noise:
    distribution: uniform
    min: 0.05
    max: 0.3
  exploration_noise_final:
    distribution: uniform
    min: 0.01
    max: 0.1
  exploration_noise_decay_steps:
    distribution: q_log_uniform_values
    min: 60000
    max: 330000
    q: 500
  warmup_steps:
    distribution: q_log_uniform_values
    min: 1500
    max: 5000
    q: 100
  buffer_size:
    distribution: q_log_uniform_values
    min: 50000
    max: 150000
    q: 1000
  batch_size:
    values: [128, 192, 256]
  updates_per_step:
    values: [1, 2, 3]
  initial_speed:
    distribution: uniform
    min: 0.4
    max: 1.0
  prevent_reverse:
    values: [true, false]
  prevent_reverse_min_speed:
    distribution: uniform
    min: 0.0
    max: 0.05
  use_per:
    values: [true, false]
  per_alpha:
    distribution: uniform
    min: 0.45
    max: 0.65
  per_beta_start:
    distribution: uniform
    min: 0.3
    max: 0.5
  per_beta_final:
    values: [0.9, 1.0]
  per_beta_increment:
    distribution: log_uniform_values
    min: 1e-5
    max: 2e-4
  per_min_priority:
    distribution: log_uniform_values
    min: 1e-5
    max: 1e-3
  per_epsilon:
    distribution: log_uniform_values
    min: 1e-6
    max: 1e-3
  hidden_dims:
    values:
      - [256, 256, 128]
      - [512, 256, 128]
      - [256, 256, 256]
  target_crash_reward:
    distribution: uniform
    min: 35.0
    max: 75.0
  self_collision_penalty:
    distribution: uniform
    min: -70.0
    max: -25.0
  truncation_penalty:
    distribution: uniform
    min: -60.0
    max: -20.0
  pressure_distance:
    distribution: uniform
    min: 0.7
    max: 1.3
  pressure_timeout:
    distribution: uniform
    min: 0.2
    max: 0.8
  pressure_min_speed:
    distribution: uniform
    min: 0.15
    max: 0.4
  pressure_bonus:
    distribution: uniform
    min: 0.01
    max: 0.12
  pressure_bonus_interval:
    values: [2, 4, 6]
  speed_bonus_coef:
    distribution: uniform
    min: 0.02
    max: 0.08
  speed_bonus_target:
    distribution: uniform
    min: 0.3
    max: 0.8
  distance_reward_near:
    distribution: uniform
    min: 0.02
    max: 0.07
  distance_reward_near_distance:
    distribution: uniform
    min: 0.4
    max: 1.0
  distance_reward_far_distance:
    distribution: uniform
    min: 1.3
    max: 3.0
  distance_penalty_far:
    distribution: uniform
    min: 0.0
    max: 0.03
  distance_gradient_scale:
    distribution: uniform
    min: 0.4
    max: 1.6
  distance_gradient_inner_value:
    distribution: uniform
    min: 0.04
    max: 0.12
  distance_gradient_outer_value:
    distribution: uniform
    min: -0.08
    max: -0.02
  step_reward:
    distribution: uniform
    min: -0.0015
    max: 0.0015
