name: gaplock_sac_hyperband
description: >
  SAC attacker sweep for the gaplock scenario. Tunes actor/critic/entropy
  learning rates, temperature targets, and reward shaping while Hyperband
  prunes low-return configs.
method: bayes
project: marl-f110
entity: ahoop004-old-dominion-university
metric:
  name: train/return
  goal: maximize

command:
  - ${env}
  - python
  - -m
  - experiments.main
  - --config
  - scenarios/gaplock_sac.yaml
  - --mode
  - train
  - --wandb
  - --wandb-project
  - marl-f110
  - --wandb-entity
  - ahoop004-old-dominion-university
  - --wandb-group
  - gaplock-sac-sweep
  - --wandb-tags
  - gaplock
  - sac
  - hyperband

early_terminate:
  type: hyperband
  min_iter: 50
  max_iter: 500
  s: 2
  eta: 3

parameters:
  agents:
    parameters:
      car_0:
        parameters:
          algorithm:
            parameters:
              params:
                parameters:
                  actor_lr:
                    distribution: log_uniform
                    min: 0.0002
                    max: 0.001
                  critic_lr:
                    distribution: log_uniform
                    min: 0.0003
                    max: 0.002
                  alpha_lr:
                    distribution: log_uniform
                    min: 0.0001
                    max: 0.0007
                  tau:
                    values: [0.0025, 0.005, 0.01]
                  batch_size:
                    values: [256, 384, 512]
                  buffer_size:
                    values: [200000, 250000, 300000]
                  target_entropy:
                    values: [-1.5, -2.0, -2.5]
                  exploration_noise:
                    values: [0.1, 0.2, 0.3]
                  exploration_noise_decay_steps:
                    values: [300000, 500000, 800000]
                  initial_speed_warmup_throttle:
                    values: [0.4, 0.6, 0.8]
                  initial_speed_warmup_steps:
                    values: [60, 120, 180]
          reward:
            parameters:
              params:
                parameters:
                  target_crash_reward:
                    values: [80.0, 100.0, 120.0]
                  truncation_penalty:
                    values: [-1.0, -5.0, -10.0]
                  pressure_timeout:
                    values: [0.35, 0.5, 0.65]
                  pressure_min_speed:
                    values: [0.2, 0.25, 0.35]
                  pressure_bonus:
                    values: [0.01, 0.015, 0.02]
                  distance_reward_near:
                    values: [0.02, 0.025, 0.03]
