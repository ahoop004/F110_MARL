name: gaplock_rainbow_stable
description: Stable Rainbow DQN hyperparameter sweep with constrained ranges to prevent catastrophic forgetting

method: bayes
metric:
  name: car_0/rolling/success_rate
  goal: maximize

project: marl-f110
entity: ahoop004-old-dominion-university

command:
  - ${env}
  - python
  - run_sweep.py
  - --scenario
  - scenarios/v2/gaplock_rainbow.yaml

early_terminate:
  type: hyperband
  min_iter: 300
  max_iter: 1000
  s: 2
  eta: 3

parameters:
  episodes:
    value: 1000

  seed:
    distribution: int_uniform
    min: 0
    max: 999999

  # Learning rate
  agents.car_0.params.lr:
    distribution: log_uniform_values
    min: 0.0001
    max: 0.0005

  # Noisy Networks exploration (reduced for stability)
  agents.car_0.params.noisy_sigma0:
    distribution: uniform
    min: 0.2
    max: 0.5

  # Multi-step returns (capped at 3 for stability)
  agents.car_0.params.n_step:
    distribution: int_uniform
    min: 1
    max: 3

  # Distributional RL - number of atoms (standard Rainbow uses 51)
  agents.car_0.params.atoms:
    distribution: q_uniform
    min: 31
    max: 51
    q: 10

  # Distributional RL - value range (tightened to actual reward range)
  agents.car_0.params.v_min:
    distribution: uniform
    min: -200.0
    max: -100.0

  agents.car_0.params.v_max:
    distribution: uniform
    min: 200.0
    max: 300.0

  # Target network update frequency (increased for stability)
  agents.car_0.params.target_update_interval:
    distribution: int_uniform
    min: 1000
    max: 2500

  # PER alpha (reduced to prevent overfitting to failures)
  agents.car_0.params.per_alpha:
    distribution: uniform
    min: 0.4
    max: 0.6

  # Success buffer ratio (how much of each batch comes from successful episodes)
  agents.car_0.params.success_buffer_ratio:
    distribution: uniform
    min: 0.3
    max: 0.6
