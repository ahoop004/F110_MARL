name: gaplock_rainbow_dqn_hyperband
description: >
  Rainbow DQN baseline sweep for the gaplock attacker. Explores discrete control
  sets, epsilon schedules, and replay/target update knobs while Hyperband trims
  weak configs using train/return.
method: bayes
project: marl-f110
entity: ahoop004-old-dominion-university
metric:
  name: train/return
  goal: maximize

command:
  - ${env}
  - python
  - -m
  - experiments.main
  - --config
  - scenarios/gaplock_rainbow_dqn.yaml
  - --mode
  - train
  - --wandb
  - --wandb-project
  - marl-f110
  - --wandb-entity
  - ahoop004-old-dominion-university
  - --wandb-group
  - gaplock-rainbow-sweep
  - --wandb-tags
  - gaplock
  - rainbow-dqn
  - hyperband

early_terminate:
  type: hyperband
  min_iter: 50
  max_iter: 500
  s: 2
  eta: 3

parameters:
  agents:
    parameters:
      car_0:
        parameters:
          algorithm:
            parameters:
              params:
                parameters:
                  lr:
                    distribution: uniform
                    min: 1e-5
                    max: 3e-3
                  gamma:
                    values: [0.97, 0.99, 0.995]
                  batch_size:
                    values: [128, 192, 256]
                  buffer_size:
                    values: [150000, 200000, 250000]
                  target_update_interval:
                    values: [500, 750, 1000]
                  epsilon_decay:
                    values: [200000, 300000, 400000]
                  epsilon_end:
                    values: [0.02, 0.05, 0.1]
                  noisy_std:
                    values: [0.3, 0.4, 0.5]
                  action_repeat:
                    values: [1, 2, 3]
                  action_set:
                    values:
                      - [[-0.35, 0.90], [-0.15, 0.80], [0.00, 0.80], [0.15, 0.80], [0.35, 0.90],
                         [-0.20, 0.30], [0.00, 0.30], [0.20, 0.30], [0.00, 0.00], [0.00, -0.50]]
                      - [[-0.35, 0.80], [-0.10, 0.70], [0.10, 0.70], [0.35, 0.80], [-0.25, 0.20],
                         [0.00, 0.20], [0.25, 0.20], [0.00, -0.20], [0.00, -0.60], [0.00, 0.00]]
                  per_beta_increment:
                    values: [5e-5, 1e-4, 2e-4]
                  per_alpha:
                    values: [0.5, 0.6, 0.7]
                  prioritized_replay:
                    values: [true]
          reward:
            parameters:
              params:
                parameters:
                  target_crash_reward:
                    values: [80.0, 100.0, 120.0]
                  truncation_penalty:
                    values: [-1.0, -5.0, -10.0]
                  pressure_timeout:
                    values: [0.35, 0.5, 0.65]
                  pressure_min_speed:
                    values: [0.2, 0.25, 0.35]
                  pressure_bonus:
                    values: [0.01, 0.015, 0.02]
                  distance_reward_near:
                    values: [0.02, 0.025, 0.03]
