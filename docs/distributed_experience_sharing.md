# Distributed Experience Sharing

Enable multiple parallel training runs to share replay buffer experiences for improved sample efficiency and exploration.

## ğŸ¯ What is Distributed Experience Sharing?

Instead of each training run learning only from its own experiences, multiple parallel runs can **cross-sample** from each other's replay buffers. This provides:

- **Improved Sample Efficiency**: Learn from diverse experiences collected by different policies
- **Better Exploration**: Leverage exploration from multiple agents simultaneously
- **Faster Learning**: Access to more diverse data accelerates convergence
- **Robustness**: Exposure to different training trajectories improves generalization

## ğŸ“Š Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Distributed Buffer Registry (Shared)              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ Buffer 1     â”‚  â”‚ Buffer 2     â”‚  â”‚ Buffer 3     â”‚     â”‚
â”‚  â”‚ (SAC Run 1)  â”‚  â”‚ (SAC Run 2)  â”‚  â”‚ (TD3 Run 1)  â”‚     â”‚
â”‚  â”‚ 1M Trans.    â”‚  â”‚ 1M Trans.    â”‚  â”‚ 1M Trans.    â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â–²                 â–²                 â–²
         â”‚                 â”‚                 â”‚
         â”‚ Sample (80%     â”‚ Sample (80%     â”‚ Sample (80%
         â”‚  own, 20%       â”‚  own, 20%       â”‚  own, 20%
         â”‚  others)        â”‚  others)        â”‚  others)
         â”‚                 â”‚                 â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â”‚ Agent 1 â”‚       â”‚ Agent 2 â”‚      â”‚ Agent 3 â”‚
    â”‚  (SAC)  â”‚       â”‚  (SAC)  â”‚      â”‚  (TD3)  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Option 1: Use the Parallel Launcher (Easiest)

Launch 4 parallel SAC runs that share experiences:

```bash
python run_parallel_distributed.py \
    --scenario scenarios/sac.yaml \
    --n_parallel 4 \
    --cross_sample_ratio 0.2 \
    --strategy self_heavy \
    --wandb
```

**Parameters:**
- `--n_parallel 4`: Launch 4 parallel training runs
- `--cross_sample_ratio 0.2`: 20% of samples come from other buffers, 80% from own
- `--strategy self_heavy`: Sample 80% from own buffer, 20% from others
- `--wandb`: Enable WandB logging for all runs

### Option 2: Manual Integration

For more control, integrate into your training script:

```python
from src.replay import create_distributed_registry, DistributedReplayBuffer

# 1. Create shared registry (usually in main process)
registry = create_distributed_registry(max_buffer_size=1000000)

# 2. Register this run's buffer
buffer_id = registry.register_buffer(run_id="my_run_1", algorithm="SAC")

# 3. Create distributed replay buffer
replay_buffer = DistributedReplayBuffer(
    buffer_size=1000000,
    observation_space=env.observation_space,
    action_space=env.action_space,
    device='cuda',
    registry=registry,
    buffer_id=buffer_id,
    sample_strategy='self_heavy',
    cross_sample_ratio=0.2,
)

# 4. Use with SB3 model
model = SAC(
    "MlpPolicy",
    env,
    replay_buffer_class=DistributedReplayBuffer,
    replay_buffer_kwargs={
        'registry': registry,
        'buffer_id': buffer_id,
        'sample_strategy': 'self_heavy',
        'cross_sample_ratio': 0.2,
    },
)

# 5. Train normally - sampling happens automatically
model.learn(total_timesteps=1000000)

# 6. Cleanup when done
registry.deregister_buffer(buffer_id)
registry.stop()
```

## âš™ï¸ Configuration

### Sampling Strategies

Choose how to sample from the distributed pool:

#### 1. **`self_heavy`** (Recommended)
- 80% from own buffer, 20% from others
- Good balance of exploitation (own policy) and exploration (others)
- Best for stable learning with diversity boost

```bash
--strategy self_heavy
```

#### 2. **`uniform`**
- Sample uniformly across all buffers
- Maximum diversity, but may destabilize own policy
- Good for exploration-heavy tasks

```bash
--strategy uniform
```

#### 3. **`weighted`**
- Sample proportional to buffer sizes
- Larger buffers contribute more samples
- Good when some runs are further along

```bash
--strategy weighted
```

#### 4. **`newest`**
- Prefer recent transitions across all buffers
- Good for non-stationary environments
- Useful when older experiences become less relevant

```bash
--strategy newest
```

#### 5. **`local_only`** (Baseline)
- Disable distributed sampling, use only own buffer
- Use for baseline comparisons

```bash
--strategy local_only
```

### Cross-Sample Ratio

Controls what fraction of mini-batches come from the distributed pool:

- `0.0`: No distributed sampling (same as `local_only`)
- `0.2`: 20% of batches from distributed pool (recommended)
- `0.5`: 50% of batches from distributed pool (high diversity)
- `1.0`: All batches from distributed pool (maximum sharing)

```bash
# Conservative (20% sharing)
--cross_sample_ratio 0.2

# Aggressive (50% sharing)
--cross_sample_ratio 0.5
```

**Recommendation**: Start with `0.2` and increase if learning is stable.

## ğŸ® Advanced Usage

### Multi-Algorithm Comparison

Run different algorithms that share experiences:

```bash
python run_parallel_distributed.py \
    --scenarios scenarios/sac.yaml scenarios/td3.yaml scenarios/ppo.yaml \
    --cross_sample_ratio 0.3 \
    --strategy weighted
```

Each algorithm sees diverse experiences from the others!

### GPU Distribution

Assign runs to specific GPUs:

```bash
python run_parallel_distributed.py \
    --scenario scenarios/sac.yaml \
    --n_parallel 4 \
    --gpus 0 1 0 1  # Runs 1,3 on GPU 0, runs 2,4 on GPU 1
```

### Monitor Buffer Statistics

```python
# In your training loop
stats = replay_buffer.get_stats()
print(f"Local samples: {stats['local_samples']}")
print(f"Distributed samples: {stats['distributed_samples']}")
print(f"Distributed ratio: {stats['distributed_ratio']:.2%}")
print(f"Number of shared buffers: {stats['num_shared_buffers']}")
```

## ğŸ“ˆ Expected Benefits

### Sample Efficiency
- **10-30% faster convergence** with `cross_sample_ratio=0.2`
- **Better final performance** due to diverse experiences

### Exploration
- Access to experiences from different exploration strategies
- Helps escape local optima
- Reduces sensitivity to initialization

### Robustness
- Exposure to diverse trajectories improves generalization
- Less likely to overfit to single policy's experiences

## âš ï¸ Considerations

### When to Use
âœ… **Use distributed sharing when:**
- Training off-policy algorithms (SAC, TD3, DDPG, DQN)
- You have multiple GPUs/cores available
- Sample efficiency is critical
- You want diverse exploration

âŒ **Don't use distributed sharing when:**
- Training on-policy algorithms (PPO, A2C) - they need on-policy data
- You have limited compute resources (overhead not worth it)
- Running quick experiments (< 500 episodes)

### Performance Overhead
- **Memory**: Each buffer uses ~1-4GB depending on buffer_size
- **CPU**: Minimal (<5%) for registry management
- **Synchronization**: Managed automatically, negligible impact

### Buffer Lifecycle
- Buffers are automatically cleaned up after 5 minutes of inactivity
- Manual cleanup via `registry.deregister_buffer(buffer_id)`
- Stale buffers don't impact active training runs

## ğŸ§ª Experimental Comparisons

Compare distributed vs local-only:

```bash
# Baseline (no sharing)
python run_parallel_distributed.py \
    --scenario scenarios/sac.yaml \
    --n_parallel 4 \
    --strategy local_only \
    --wandb

# With sharing
python run_parallel_distributed.py \
    --scenario scenarios/sac.yaml \
    --n_parallel 4 \
    --strategy self_heavy \
    --cross_sample_ratio 0.2 \
    --wandb
```

Compare learning curves in WandB to quantify the benefit!

## ğŸ” Troubleshooting

### "Registry not available" errors
- Ensure registry is created and started before training
- Check that `buffer_id` is registered
- Verify registry isn't stopped prematurely

### High memory usage
- Reduce `buffer_size` (default 1M is large)
- Reduce `n_parallel` runs
- Use `cross_sample_ratio < 0.3` to reduce memory pressure

### Unstable training with distributed sampling
- Reduce `cross_sample_ratio` (try 0.1)
- Use `strategy='self_heavy'` instead of `'uniform'`
- Ensure all runs use same reward normalization

### Processes not cleaning up
- Use Ctrl+C to gracefully terminate
- Check for zombie processes: `ps aux | grep run_parallel`
- Kill manually if needed: `pkill -f run_parallel`

## ğŸ“š References

This implementation is inspired by:

1. **Distributed Prioritized Experience Replay** (Horgan et al., 2018)
2. **Ape-X** (Distributed Prioritized Experience Replay, DeepMind)
3. **Population-Based Training** (Jaderberg et al., 2017)

## ğŸ’¡ Tips

1. **Start Conservative**: Use `cross_sample_ratio=0.2` and `strategy='self_heavy'`
2. **Monitor Stats**: Check `distributed_ratio` to verify sharing is happening
3. **Compare Baselines**: Always compare to `local_only` to quantify benefit
4. **Scale Gradually**: Start with 2-3 parallel runs, scale up if beneficial
5. **Same Algorithm**: Use same algorithm for all runs initially, mix algorithms once stable

## ğŸ¯ Best Practices

### For Research
```bash
# Run ablation study
python run_parallel_distributed.py \
    --scenario scenarios/sac.yaml \
    --n_parallel 3 \
    --strategy self_heavy \
    --cross_sample_ratio 0.2 \
    --wandb --extra_args --tag distributed_20pct

python run_parallel_distributed.py \
    --scenario scenarios/sac.yaml \
    --n_parallel 3 \
    --strategy local_only \
    --wandb --extra_args --tag baseline_local
```

### For Production
```bash
# Reliable configuration for best results
python run_parallel_distributed.py \
    --scenario scenarios/sac.yaml \
    --n_parallel 4 \
    --strategy self_heavy \
    --cross_sample_ratio 0.15 \
    --gpus 0 1 2 3 \
    --wandb
```

---

**Questions?** Check the code documentation in `src/replay/` or open an issue!
