# config.yaml
bridge:
  ros__parameters:
    # topics and namespaces
    ego_namespace: 'ego_racecar'
    ego_scan_topic: 'scan'
    ego_odom_topic: 'odom'
    ego_opp_odom_topic: 'opp_odom'
    ego_drive_topic: 'drive'
    opp_namespace: 'opp_racecar'
    opp_scan_topic: 'opp_scan'
    opp_odom_topic: 'odom'
    opp_ego_odom_topic: 'opp_odom'
    opp_drive_topic: 'opp_drive'

    # transform related
    scan_distance_to_base_link: 0.0
    
    # laserscan parameters
    scan_fov: 4.7
    scan_beams: 1080

    # map parameters
    map_path: '/home/aaron/f110_gymnasium_ros2_jazzy/assets/maps/Shanghai_map'
    map_img_ext: '.png'
    centerline_path: '/home/aaron/f110_gymnasium_ros2_jazzy/assets/racelines/Shanghai_waypoints.csv'

    # opponent parameters
    num_agent: 2

    model: '/home/aaron/f110_gymnasium_ros2_jazzy/rl_training/TD3/models'

    # ego starting pose on map
    sx: 0.0
    sy: 0.0
    stheta: 0.0

    # opp starting pose on map
    sx1: 2.0
    sy1: 0.5
    stheta1: 0.0

    ego_controller: gap_follow    
    opp_controller: gap_follow    
    kb_teleop: false


env:
  id: "f110_gym:f110-v0"
  render_mode: "human_fast"  
  map_path: '/home/aaron/f110_gymnasium_ros2_jazzy/assets/maps/Shanghai_map'
  map_dir: "/home/aaron/f110_gymnasium_ros2_jazzy/assets/maps"           # <- change me
  map: "/home/aaron/f110_gymnasium_ros2_jazzy/assets/maps/Shanghai_map"                # <- change me
  map_ext: ".png"
  num_agents: 2
  max_steps: 1500
  seed: 42

  model: '/home/aaron/f110_gymnasium_ros2_jazzy/rl_training/TD3/models/'


  # Start poses for [ego, opp] as [x, y, yaw] in meters/radians.
  # Replace with your track-specific values.
  start_poses:
    - - [0.0, 0.0, 0.0]    # ego
      - [1.5, 0.5, 0.0]    # opp
    - - [0.0, 0.0, 0.0]
      - [1.5,-0.3, 0.0]
    - - [0.0, 0.0, 0.0]
      - [1.5, 0.0, 0.0]
    - - [0.0, 0.5, 0.0]
      - [1.5,-0.3, 0.0]
    - - [0.0,-0.3, 0.0]
      - [1.5, 0.5, 0.0]
    - - [0.0, 0.0, 0.0]
      - [1.5, 0.0, 0.26]
    - - [0.0, 0.0, 0.0]
      - [1.5, 0.0,-0.26]
    - - [0.0, 0.0, 0.0]
      - [1.5, 1.0, 0.17]
    - - [0.0, 0.0, 0.0]
      - [1.5,-0.1,-0.17]
    - - [0.0, 0.0, 0.17]
      - [1.5, 0.0, 0.0]
    - - [0.0, 0.0,-0.17]
      - [1.5, 0.0, 0.0]
    - - [0.0, 0.0, 0.0]
      - [1.5, 0.0, 0.0]
    - - [0.0, 0.0, 0.0]
      - [-1.5, 0.5, 0.0]
    - - [0.0, 0.0, 0.0]
      - [-1.5, 0.0, 0.0]
    - - [0.0, 0.0, 0.0]
      - [-1.5, 0.0, 0.0]
    - - [0.0, 0.0, 0.0]
      - [-1.5,-0.3, 0.0]
    - - [1.5, 0.5, 0.0]    # opp
      - [0.0, 0.0, 0.0]    # ego
    - - [1.5,-0.3, 0.0]
      - [0.0, 0.0, 0.0]
    - - [1.5, 0.0, 0.0]
      - [0.0, 0.0, 0.0]
    - - [1.5,-0.3, 0.0]
      - [0.0, 0.5, 0.0]
    - - [1.5, 0.5, 0.0]
      - [0.0,-0.3, 0.0]
    - - [1.5, 0.0, 0.26]
      - [0.0, 0.0, 0.0]
    - - [1.5, 0.0,-0.26]
      - [0.0, 0.0, 0.0]
    - - [1.5, 1.0, 0.17]
      - [0.0, 0.0, 0.0]
    - - [1.5,-0.1,-0.17]
      - [0.0, 0.0, 0.0]
    - - [1.5, 0.0, 0.0]
      - [0.0, 0.0, 0.17]
    - - [1.5, 0.0, 0.0]
      - [0.0, 0.0,-0.17]
    - - [1.5, 0.0, 0.0]
      - [0.0, 0.0, 0.0]
    - - [1.5, 0.5, 0.0]
      - [0.0, 0.0, 0.0]
    - - [1.5, 0.0, 0.0]
      - [0.0, 0.0, 0.0]
    - - [1.5, 0.5, 0.0]
      - [0.0, 0.0, 0.0]
    - - [1.5,-0.3, 0.0]
      - [0.0, 0.0, 0.0]


  # Action bounds used by your env; keep speed forward-only to start.
  action_low:  [-0.4189, -1.5]        # [steer_min(rad), speed_min(m/s)]
  action_high: [ 0.4189, 10.0]        # [steer_max(rad), speed_max(m/s)]

  # LiDAR geometry used by env/bridge
  lidar:
    beams: 1080
    fov: 4.7                         # radians
    range_max_m: 30.0                # you updated the env to 30 m

obs:

  lidar_max: 30.0
         # floor for auto mode (ignored if fixed)

action:
  # Exploration noise on actor output before mapping to env bounds
  train_action_noise_std: 0.03       # in normalized [-1,1] space
  train_action_noise_clip: 0.15
  # Target policy smoothing for TD3 (applied to target actions)
  policy_noise: 0.10
  noise_clip: 0.10


td3:
  actor_hidden: [128, 128]
  critic_hidden: [128, 128]
  gamma: 0.99
  tau: 0.05
  actor_lr: 3.0e-4
  critic_lr: 3.0e-4
  policy_freq: 2
  # (policy_noise/noise_clip are in action: theyâ€™re also used in the agent)


per:
  alpha: 0.0
  beta_init: 0.4
  beta_final: 1.0
  capacity: 1000000
  priority_epsilon: 1.0e-6

train:
  total_steps: 1000000
  warmup_steps: 10000
  update_after: 1000
  batch_size: 128
  updates_per_step: 1
  eval_every_steps: 10000
  save_every_steps: 50000
  seed: 42
  render: true

paths:
  run_name: "td3_limo_base"
  checkpoints_dir: "./checkpoints"
  logs_dir: "./logs"

logging:
  enable_tensorboard: false
  enable_wandb: false
  project: "td3-limo"
  entity: ""                          # your wandb username/org if used
  print_every_steps: 1000
  video_every_eval: 0                 # 0 to disable
