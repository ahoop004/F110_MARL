# TODO

- [x] Refactor config handling using ExperimentConfig dataclasses.
- [x] Extract env/agent factory utilities so train/eval share a common build API (see ticket).
  - [x] Update map_loader.py to centralize map metadata augmentation.
  - [x] Flesh out start_pose.py with parsing/adjust/reset helpers.
    - [x] Validate spawn poses against map (inside drivable area).
  - [x] Add builders module returning env + agents + wrappers.
    - [x] Define factory function to load cfg, map, env, start poses.
    - [x] Ensure function returns env, map_data, start_pose options for callers.
    - [x] Introduce agent spec registry supporting PPO, TD3, DQN, heuristic policies.
    - [x] Support composing mixed teams (â‰¤4 agents) driven by config roles.
    - [x] Provide wrapper factory configurable per agent algorithm.
    - [x] Document expected return types / usage.
  - [x] Refactor train.py to consume the new builders.
  - [x] Refactor eval.py (and main.py) to consume the new builders.
  - [x] Smoke-test train/eval after refactor.
- [x] Wrap PPO logic in a generic Trainer interface; plan for plugging in other agents (SAC, TD3).
  - [x] Define transition-centric Trainer protocol (select_action, observe, update, save/load).
  - [x] Introduce reusable Transition dataclass (obs, action, reward, next_obs, done, info).
  - [x] Wrap PPOAgent in a TrainerAdapter consuming transitions.
  - [x] Expose registry hook so TD3/DQN trainers can register without loop changes.
  - [ ] Implement TD3Trainer using shared protocol.
  - [ ] Implement DQNTrainer using shared protocol.
  - [ ] Register TD3/DQN builders and expose config knobs.
  - [ ] Update docs/examples to highlight `AgentTeam` + trainer usage.
- [x] Standardize evaluation wrapper with deterministic actions and richer metrics (collision counts, lap stats).
  - [x] Capture metrics (episode length, collisions, lap counts).
  - [x] Log deterministic eval results to wandb/console.
  - [x] Optionally save evaluation rollouts for playback.
- [x] Integrate structured logging (wandb/TensorBoard) for both training updates and eval runs.
  - [x] Add wandb/TensorBoard hooks for training updates.
  - [x] Log PPO losses (policy/value/entropy) each update.
  - [x] Emit eval metrics to wandb/TensorBoard.
- [ ] Update CLI/docs for new `--render`, `--episodes`, and trainer workflow.
- [ ] Build map_features utility for derived artefacts (centerline, walls, friction).
  - [ ] Generate centerline/waypoint data from MapData.
  - [ ] Extract wall/out-of-bounds masks supporting varied color schemes.
  - [ ] Define friction/track property map generation hooks.
- [ ] Prepare sweeps.yaml variants per algorithm once factories are in place.
  - [ ] Define sweeps for PPO (lr, ent_coef, clip).
            - [ ] Add SAC/TD3 sweeps once trainers exist.
