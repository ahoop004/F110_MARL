# TODO

- [x] Refactor RewardWrapper: extract Position-2 math + normalization into helper module; centralize coefficient definitions.
- [ ] Consolidate observation/action wrapper utilities (shared LiDAR downsample, to_numpy helpers, observation pipeline abstraction).
- [ ] Introduce typed config schema (pydantic or custom) so env/agent/reward defaults live in one place.
- [ ] Split F110ParallelEnv into submodules (LiDAR, collision state machine, start-poses) for readability/testing.
- [ ] Create shared CLI utilities (yaml loading, logging, manifest helpers) for scripts/run.py/map_validator.py.
- [ ] Expand README and supporting docs to explain env/map builders, agent roster wiring, and trainer handoff (PPO/TD3/DQN) so newcomers can launch experiments without diving into code.
- [ ] Add automated tests exercising trainer/policy update loops and config parsing (builders, wrappers, reward wiring) to catch regressions beyond the current env smoke tests.
- [ ] Document RewardWrapper lifecycle expectations (reset usage, crash bookkeeping) and ensure trainers call it consistently to avoid state leakage.
- [ ] Break up F110ParallelEnv and physics helpers with clearer module boundaries or shape/type docstrings, improving readability around lidar/collision pipelines.
- [ ] Prepare CLI guidance for the trainer registry workflow once doc/tests land, including example commands for common training/eval flows.
- [ ] Relocate checkpoints/eval artifacts to an ignored outputs/ directory and add helper script to bundle config + git SHA per run.
    - [ ] Add manifest writer (CSV/JSON) capturing run_id, algo, map, seed, output_dir, git_sha.
    - [ ] Update `.gitignore` for outputs/ and retrofit existing scripts to use the new directory.
- [ ] Document reward normalization (reward_horizon/reward_clip) behaviour and guard against accidental max_step changes across sweeps.
- [ ] Let the map validator whitelist known-good assets or tune thresholds so sweep dashboards aren't flooded with wall-band warnings.
- [ ] Consider wiring the `lidar_beams` option into PPO/TD3 wrappers (or document why they keep full scans) for consistent obs shapes.
- [ ] DQN: derive the discrete `action_set` from vehicle steering/accel bounds (`src/f110x/utils/builders.py`, `src/f110x/policies/dqn/dqn.py`) by meshing steering bins (`params['s_min']`, `params['s_max']`, `Î”s`) with accel/brake bins `[-params['a_max'], 0.0, 0.4*params['a_max'], params['a_max']]` to produce 20 actions and expose a `decode_action(idx) -> (steer_delta, accel)` helper.
- [ ] DQN: add K-step action hold inside `src/f110x/envs/f110ParallelEnv.py` (track `hold_left`, replay last command until it expires, enforce steering rate limits and keep velocities within `[params['v_min'], params['v_max']]`).
- [ ] DQN: extend the observation adapter (likely `obs` wrapper in `src/f110x/wrappers/observation.py`) to append an action-mask vector keyed to the discrete table (respect hold window sizes `w_s`, `w_a`, and physics guards when steering/velocity saturate).
- [ ] Resource planning for massive sweeps.
    - [ ] Draft compute concurrency plan (runs per GPU/CPU, reserved debug slot).
    - [ ] Specify storage budget + retention policy (outputs/<algo>/<map>/<seed>/ layout).
    - [ ] Add submission helper (scripts/launch_array.py or similar) with retry/status tracking.
    - [ ] Set up monitoring/alerting (W&B dashboards, log watchdog).
