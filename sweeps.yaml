program: main.py
method: random
metric:
  name: eval/return_ppo
  goal: maximize
parameters:
  ppo.actor_lr:
    values: [0.0001, 0.0003, 0.0005]
  ppo.ent_coef:
    values: [0.0, 0.005, 0.01]
  ppo.clip_eps:
    values: [0.1, 0.2, 0.3]
  reward.herd_bonus:
    values: [150.0, 200.0, 250.0]
  reward.ego_collision_penalty:
    values: [-200.0, -300.0, -400.0]
  env.start_pose_options:
    values:
      - [[0.0, 0.0, 0.0], [2.0, 0.5, 0.0]]
      - [[0.0, -0.5, 0.0], [2.5, 0.5, 0.0]]
overrides:
  wandb:
    value: true
  mode:
    value: train
