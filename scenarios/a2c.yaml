includes:
- ../configs/env/line2_gaplock.yaml
- ../configs/reward/gaplock_attacker.yaml
- ../configs/agents/ftg_defender.yaml
- ../configs/curriculum/line2_gaplock_phased.yaml
- ../configs/evaluation/line2_gaplock_eval_standard.yaml
- ../configs/wandb.yaml
experiment:
  name: a2c
  episodes: 1500
  seed: 42
environment:
  max_steps: 2500
agents:
  car_0:
    role: attacker
    algorithm: sb3_a2c
    target_id: car_1
    params:
      learning_rate: 0.0003
      gamma: 0.995
      n_steps: 5
      gae_lambda: 1.0
      ent_coef: 0.0
      vf_coef: 0.5
      hidden_dims:
      - 256
      - 256
    observation:
      preset: gaplock
    reward:
      preset: gaplock_full
      overrides:
        forcing:
          enabled: true
          pinch_pockets:
            enabled: true
            anchor_forward: 1.2
            anchor_lateral: 0.35
            sigma: 0.55
            weight: 0.004
            peak: 1.0
            floor: -0.5
            power: 2.0
          clearance:
            enabled: true
            weight: 0.003
            band_min: 0.3
            band_max: 3.2
            clip: 0.015
            time_scaled: true
          turn:
            enabled: true
            weight: 0.006
            clip: 0.015
            time_scaled: true
        distance:
          enabled: false
          near_distance: 1.0
          far_distance: 2.5
          reward_near: 0.004
          penalty_far: 0.002
        heading:
          enabled: true
          coefficient: 0.001
        speed:
          enabled: true
          bonus_coef: 0.0005
        step_reward: -0.001
        terminal:
          target_crash: 50.0  # Success bonus (was 200.0 - too large, causing training instability)
          self_crash: -40.0   # Failure penalty (was -20.0 - too lenient)
          timeout: -15.0      # Timeout penalty (was -100.0 - too severe)
          collision: 0.0      # Handled via self_crash/target_crash
  car_1:
    role: defender
    algorithm: ftg
    params:
      max_distance: 12.0
      window_size: 4
      bubble_radius: 3
      max_steer: 0.42
      min_speed: 0.2
      max_speed: 1.0
      steering_gain: 0.35
      fov: 4.71238898
      normalized: false
      steer_smooth: 0.6
      mode: lidar
      gap_min_range: 0.4
      target_mode: center
      wall_avoid_kick: 0.02
      panic_factor_near: 1.0
      panic_factor_very_near: 1.0
      use_disparity_extender: true
      disparity_threshold: 0.35
      vehicle_width: 0.225
      safety_margin: 0.08
      no_cutback_enabled: true
      cutback_clearance: 0.9
      cutback_hold_steps: 8
    ftg_schedule:
      enabled: true
      by_stage:
        optimal_varied_speed:
          max_speed: 0.6
          steering_gain: 0.4
          steer_smooth: 0.4
          bubble_radius: 2.5
          safety_margin: 0.1
          gap_min_range: 0.55
          target_mode: center
          wall_avoid_kick: 0.05
          panic_factor_near: 1.1
          panic_factor_very_near: 1.25
        full_random:
          max_speed: 1.0
          steering_gain: 0.6
          steer_smooth: 0.3
          bubble_radius: 3.0
          safety_margin: 0.15
          gap_min_range: 0.65
          target_mode: farthest
          wall_avoid_kick: 0.12
          panic_factor_near: 1.2
          panic_factor_very_near: 1.4

# Evaluation configuration (optional)
# Uncomment to enable periodic evaluation during training
evaluation:
  ftg_override:
    steering_gain: 0.35
wandb:
  enabled: true
  project: marl-f110
  entity: ahoop004-old-dominion-university
  name: null
  group: null
  job_type: null
  tags:
  - a2c
  - comparison
  - gaplock
  notes: "A2C comparison for gaplock adversarial task (use run_sb3.py)"
  mode: online
