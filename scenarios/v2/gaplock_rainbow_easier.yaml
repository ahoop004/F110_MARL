# Gaplock Rainbow DQN - Easier Training Configuration
# Uses discrete actions (steering x speed combinations)

experiment:
  name: gaplock_rainbow_easier
  episodes: 1500
  seed: 42

environment:
  map: maps/line2/line2.yaml
  num_agents: 2
  max_steps: 2500
  lidar_beams: 720
  spawn_points: [spawn_2, spawn_1]
  timestep: 0.01
  render: false

  # Spawn curriculum - easier progression
  spawn_curriculum:
    enabled: true
    window: 200
    activation_samples: 50
    min_episode: 50
    enable_patience: 5
    disable_patience: 3
    cooldown: 20
    lock_speed_steps: 175

    stages:
      - name: "close_slow"
        spawn_points: [spawn_pinch_ahead]
        speed_range: [0.4, 0.4]
        enable_rate: 0.50

      - name: "close_varied"
        spawn_points: [spawn_pinch_ahead, spawn_pinch_right, spawn_pinch_left]
        speed_range: [0.4, 0.7]
        enable_rate: 0.60
        disable_rate: 0.40

      - name: "full_random"
        spawn_points: "all"
        speed_range: [0.3, 1.0]
        disable_rate: 0.45

    spawn_configs:
      spawn_pinch_right:
        car_0: [-45.569, -1.000, 0.100]
        car_1: [-46.769, -0.700, 0.000]
      spawn_pinch_left:
        car_0: [-45.569, -0.400, -0.100]
        car_1: [-46.769, -0.700, 0.000]
      spawn_pinch_ahead:
        car_0: [-45.569, -0.700, 0.000]
        car_1: [-46.769, -0.700, 0.000]

  vehicle_params:
    mu: 1.0489
    C_Sf: 4.718
    C_Sr: 5.4562
    lf: 0.15875
    lr: 0.17145
    h: 0.074
    length: 0.32
    width: 0.225
    m: 3.74
    I: 0.04712
    s_min: -0.46
    s_max: 0.46
    sv_min: -3.2
    sv_max: 3.2
    v_switch: 0.8
    a_max: 2.0
    v_min: -1.0
    v_max: 1.0

agents:
  car_0:
    role: attacker
    algorithm: rainbow_dqn
    target_id: car_1

    params:
      # Rainbow DQN hyperparameters
      lr: 0.0003
      gamma: 0.995
      hidden_dims: [512, 256]    # Larger network for discrete Q-values
      buffer_size: 1000000
      batch_size: 256
      learning_starts: 2500
      target_update_freq: 1000   # Update target network every 1000 steps
      max_grad_norm: 10.0

      # Rainbow components
      use_double_dqn: true       # Double DQN
      use_dueling: true          # Dueling architecture
      use_noisy: true            # Noisy networks for exploration
      use_per: true              # Prioritized replay
      use_n_step: true           # Multi-step returns
      use_categorical: true      # Distributional RL (C51)

      # N-step returns
      n_step: 3                  # 3-step returns for faster credit assignment

      # Categorical DQN (C51)
      v_min: -30.0               # Min expected return
      v_max: 50.0                # Max expected return
      num_atoms: 51              # Number of distribution atoms

      # Noisy networks
      noisy_sigma: 0.5           # Noise parameter

      # Prioritized replay
      per_alpha: 0.6
      per_beta: 0.4
      per_beta_increment: 0.001
      per_epsilon: 0.01

      # Epsilon-greedy (disabled when using noisy nets)
      epsilon_start: 1.0
      epsilon_end: 0.05
      epsilon_decay_steps: 50000

      # Action discretization - 15 actions (5 steering x 3 speed)
      # Cartesian product of steering × speed
      # Format: [steering, speed] where steering ∈ [-1, 1], speed ∈ [0, 1]
      action_set:
        # Hard left steering
        - [-1.0, 0.3]   # Hard left, slow
        - [-1.0, 0.65]  # Hard left, medium
        - [-1.0, 1.0]   # Hard left, fast
        # Medium left steering
        - [-0.5, 0.3]   # Left, slow
        - [-0.5, 0.65]  # Left, medium
        - [-0.5, 1.0]   # Left, fast
        # Straight
        - [0.0, 0.3]    # Straight, slow
        - [0.0, 0.65]   # Straight, medium
        - [0.0, 1.0]    # Straight, fast
        # Medium right steering
        - [0.5, 0.3]    # Right, slow
        - [0.5, 0.65]   # Right, medium
        - [0.5, 1.0]    # Right, fast
        # Hard right steering
        - [1.0, 0.3]    # Hard right, slow
        - [1.0, 0.65]   # Hard right, medium
        - [1.0, 1.0]    # Hard right, fast

      prevent_reverse: true
      prevent_reverse_min_speed: 0.01
      prevent_reverse_speed_index: 1

    observation:
      preset: gaplock

    # SIMPLIFIED REWARD - Focus on pursuit
    reward:
      preset: gaplock_simple

      overrides:
        # PRIMARY: Distance-based pursuit reward
        distance:
          enabled: true
          near_distance: 2.0
          far_distance: 4.0
          reward_near: 0.2
          penalty_far: 0.05

        # SECONDARY: Heading alignment
        heading:
          enabled: true
          coefficient: 0.02

        # SECONDARY: Speed bonus
        speed:
          enabled: true
          bonus_coef: 0.01

        step_reward: -0.005

        # STRONG terminal rewards
        terminal:
          target_crash: 25.0
          self_crash: -10.0
          timeout: -2.0

  car_1:
    role: defender
    algorithm: ftg

    params:
      max_distance: 12.0
      window_size: 4
      bubble_radius: 3
      max_steer: 0.32

      min_speed: 0.2
      max_speed: 0.7             # Slower than attacker

      steering_gain: 0.5
      fov: 4.71238898
      normalized: false
      steer_smooth: 0.4
      mode: "lidar"
      gap_min_range: 0.65
      target_mode: "farthest"
      use_disparity_extender: true
      disparity_threshold: 0.35
      vehicle_width: 0.225
      safety_margin: 0.08
      no_cutback_enabled: true
      cutback_clearance: 0.9
      cutback_hold_steps: 8

wandb:
  enabled: false
  project: marl-f110
  entity: ahoop004-old-dominion-university
  tags: [rainbow_dqn, easier, pursuit]
  notes: "Rainbow DQN with discrete actions for pursuit task"
