# Gaplock TD3 - Easier Training Configuration
# TD3 often works better than SAC for pursuit tasks due to deterministic policy

experiment:
  name: gaplock_td3_easier
  episodes: 1500
  seed: 42

environment:
  map: maps/line2/line2.yaml
  num_agents: 2
  max_steps: 2000
  lidar_beams: 108
  spawn_points: [spawn_2, spawn_1]
  timestep: 0.01
  render: false

  # Spawn curriculum - easier progression (start with varied spawns)
  spawn_curriculum:
    enabled: true
    window: 200
    activation_samples: 50
    min_episode: 100          # Start curriculum later, let agent explore first
    enable_patience: 7         # More patient before advancing
    disable_patience: 3
    cooldown: 30               # Longer cooldown between stage changes
    lock_speed_steps: 200      # Less speed locking

    stages:
      - name: "random_slow"
        spawn_points: "all"    # Start with varied positions
        speed_range: [0.3, 0.5]  # Slow speeds first
        enable_rate: 0.65

      - name: "random_medium"
        spawn_points: "all"
        speed_range: [0.3, 0.7]  # Increase speed range
        enable_rate: 0.70
        disable_rate: 0.50

      - name: "close_challenges"
        spawn_points: [spawn_pinch_ahead, spawn_pinch_right, spawn_pinch_left]
        speed_range: [0.4, 0.8]  # Harder scenarios with pinch spawns
        enable_rate: 0.75
        disable_rate: 0.55

      - name: "full_random"
        spawn_points: "all"
        speed_range: [0.3, 1.0]
        disable_rate: 0.60

    spawn_configs:
      spawn_pinch_right:
        car_0: [-45.569, -1.000, 0.100]
        car_1: [-46.769, -0.700, 0.000]
      spawn_pinch_left:
        car_0: [-45.569, -0.400, -0.100]
        car_1: [-46.769, -0.700, 0.000]
      spawn_pinch_ahead:
        car_0: [-45.569, -0.700, 0.000]
        car_1: [-46.769, -0.700, 0.000]

  vehicle_params:
    mu: 1.0489
    C_Sf: 4.718
    C_Sr: 5.4562
    lf: 0.15875
    lr: 0.17145
    h: 0.074
    length: 0.32
    width: 0.225
    m: 3.74
    I: 0.04712
    s_min: -0.46
    s_max: 0.46
    sv_min: -3.2
    sv_max: 3.2
    v_switch: 0.8
    a_max: 2.0
    v_min: -1.0
    v_max: 1.0

agents:
  car_0:
    role: attacker
    algorithm: td3
    target_id: car_1

    params:
      # TD3 hyperparameters - conservative for stability
      lr_actor: 0.0001         # Reduced for more stable learning
      lr_critic: 0.0003        # Slightly higher for critic (typical for TD3)
      gamma: 0.99              # Reduced discount for faster credit assignment
      tau: 0.005
      policy_delay: 2          # Update policy every 2 critic updates
      hidden_dims: [256, 256]
      buffer_size: 1000000
      success_buffer_size: 200000
      success_buffer_ratio: 0.3
      batch_size: 256
      learning_starts: 10000        # Collect more diverse data before learning
      max_grad_norm: 0.5

      # Exploration noise - CRITICAL for TD3
      exploration_noise: 0.1         # Initial exploration noise
      exploration_noise_final: 0.02  # Final exploration noise (keep some exploration)
      exploration_noise_decay_steps: 100000  # Decay over 100k steps (~50 episodes at 2k steps)
      target_noise: 0.2              # Noise for target policy smoothing
      target_noise_clip: 0.5         # Clip target noise

      # Prioritized Experience Replay - ENABLED
      use_per: true
      per_alpha: 0.6
      per_beta: 0.4
      per_beta_increment: 0.001
      per_epsilon: 0.01

      prevent_reverse: true
      prevent_reverse_min_speed: 0.01
      prevent_reverse_speed_index: 1

    observation:
      preset: gaplock

    # SIMPLIFIED REWARD - Focus on pursuit
    reward:
      preset: gaplock_simple

      overrides:
        # PRIMARY: Terminal outcomes (favor clean pinches)
        terminal:
          target_crash: 100.0
          self_crash: -60.0
          collision: -120.0
          timeout: -10.0

        # PRIMARY: Pressure (stay close, stay aligned)
        pressure:
          enabled: true
          distance_threshold: 1.10
          min_speed: 0.25
          heading_tolerance: 1.00
          bonus: 0.05
          bonus_interval: 5
          streak_bonus: 0.04
          streak_cap: 20

        # PRIMARY: Distance-based shaping
        distance:
          enabled: true
          near_distance: 1.20
          far_distance: 2.50
          reward_near: 0.05
          penalty_far: 0.03

        # PRIMARY: Forcing/pinch shaping
        forcing:
          enabled: true
          pinch_pockets:
            enabled: true
            anchor_forward: 1.20
            anchor_lateral: 0.35
            sigma: 0.60
            weight: 0.05
          clearance:
            enabled: true
            weight: 0.03
            band_min: 0.30
            band_max: 3.00
            clip: 0.02
            time_scaled: true
          turn:
            enabled: true
            weight: 0.06
            clip: 0.02
            time_scaled: true

        # SECONDARY: Heading alignment (point towards target)
        heading:
          enabled: true
          coefficient: 0.004

        # SECONDARY: Speed bonus (move fast)
        speed:
          enabled: true
          bonus_coef: 0.002

        # Small step penalty
        step_reward: -0.005


  car_1:
    role: defender
    algorithm: ftg

    params:
      max_distance: 12.0
      window_size: 4
      bubble_radius: 3.5
      max_steer: 0.30

      min_speed: 0.2
      max_speed: 0.6             # Slower than attacker

      steering_gain: 0.4
      fov: 4.71238898
      normalized: false
      steer_smooth: 0.4
      mode: "lidar"
      gap_min_range: 0.65
      target_mode: "farthest"
      use_disparity_extender: true
      disparity_threshold: 0.35
      vehicle_width: 0.225
      safety_margin: 0.12
      no_cutback_enabled: true
      cutback_clearance: 1.0
      cutback_hold_steps: 8

wandb:
  enabled: true
  project: marl-f110
  entity: ahoop004-old-dominion-university
  tags: [td3, easier, pursuit]
  notes: "TD3 training with distance-based pursuit rewards"
