default_experiment: gaplock_dqn  # Default run profile selects the DQN experiment

shared:  # Common building blocks reused across experiment profiles
  env: &env_base  # Baseline simulator configuration for all experiments
    seed: 42  # PRNG seed to keep environment deterministic across runs
    n_agents: 2  # Number of simultaneous vehicles in the scene
    max_steps: 1000  # Episode length cap before truncation
    timestep: 0.01  # Physics integration step in seconds
    integrator: RK4  # Numerical integrator used by the dynamics model
    render_interval: 0  # Render every N episodes (0 disables routine rendering)
    vehicle_params:  # Overwrites for the default vehicle model
      v_min: -2.0  # Minimum longitudinal velocity allowed for the car
      v_max: 10.0  # Maximum longitudinal velocity allowed for the car
    map_dir: maps  # Directory containing track layouts
    map_yaml: Shanghai_map.yaml  # YAML file describing the selected map
    map: Shanghai_map.yaml  # Explicit map identifier used by the simulator
    map_ext: .png  # File extension for the occupancy grid associated with the map
    render_mode: human  # Visualization backend requested when rendering
    start_poses:  # Deterministic start positions for each agent
      - [0.0, 0.0, 0.0]  # Attacker spawn pose (x, y, yaw)
      - [2.0, 0.1, 0.0]  # Defender spawn pose (x, y, yaw)
    start_pose_options:  # Optional randomized start pose sets to sample from
      -  # Option A keeps the defender ahead with a slight left offset
        - [0.0, 0.0, 0.0]  # Attacker pose for option A
        - [2.5, 0.1, 0.0]  # Defender pose for option A
      -  # Option B keeps the defender ahead with a slight right offset
        - [0.0, 0.0, 0.0]  # Attacker pose for option B
        - [2.5, -0.1, 0.0]  # Defender pose for option B
      -  # Option C spawns the attacker behind the defender (left offset)
        - [2.5, 0.1, 0.0]  # Attacker pose for option C
        - [0.0, 0.0, 0.0]  # Defender pose for option C
      -  # Option D spawns the attacker behind the defender (right offset)
        - [2.5, -0.1, 0.0]  # Attacker pose for option D
        - [0.0, 0.0, 0.0]  # Defender pose for option D

  defender_policy: &defender_policy  # Static defender policy reused by all profiles
    algo: follow_gap  # Use the follow-the-gap reactive controller
    slot: 1  # Slot in the multi-agent roster reserved for the defender
    role: defender  # Semantic role label used by wrappers and logging
    params:  # Tunables specific to the follow-gap controller
      max_speed: 18.0  # Upper speed bound enforced by the controller (m/s)
      min_speed: 3.0  # Lower speed bound to avoid stopping entirely
      steering_gain: 0.85  # Proportional gain applied to steering corrections
      bubble_radius: 3  # Distance ahead to ignore obstacles when selecting gaps (m)
      steer_smooth: 0.35  # Smoothing factor applied to steering commands

  reward: &reward_base  # Sparse success reward used by every attacker
    target_crash_reward: 100.2  # Positive payout when the defender crashes and attacker does not
    ego_collision_penalty: -2.0  # Negative reward applied if the attacker crashes
    truncation_penalty: 0.0  # Optional penalty when episodes end via truncation
    success_once: true  # Only award the success bonus the first time the defender crashes
    reward_horizon: null  # Optional scaling horizon for the sparse reward (null disables scaling)
    reward_clip: null  # Optional global reward clip (null leaves rewards unclipped)
    idle_speed_threshold: 0.4  # Speed threshold leveraged by training utilities for idle detection (m/s)
    idle_patience_steps: 120  # Steps tolerated under the idle threshold before flagging stagnation

  episodes:  # Shared episode counts referenced by individual algorithms
    train: &shared_train_episodes 5000  # Number of training episodes each run targets
    eval: &shared_eval_episodes 1  # Number of evaluation rollouts per checkpoint

  wandb: &wandb_base  # Common Weights & Biases defaults
    enabled: true  # Toggle experiment tracking
    project: marl-f110  # W&B project to log under
    entity: ronnies_group  # W&B entity or team name that owns the project

  attacker_roster: &attacker_base  # Shared attacker roster template
    slot: 0  # Roster position assigned to the attacker
    role: attacker  # Semantic role for the learning agent
    wrappers: &attacker_wrappers  # Observation/action wrappers applied to the attacker
      - factory: obs  # Observation builder factory name
        params:  # Arguments forwarded to the observation wrapper
          max_scan: 30.0  # Maximum LiDAR range provided to the agent (m)
          normalize: true  # Enable normalization of observation vectors
          target_role: defender  # Role label used to select the opponent for features

  main: &main_base  # Shared runtime configuration
    mode: train_eval  # Run training followed by evaluation in one invocation
    save_eval_rollouts: true  # Persist evaluation trajectories for inspection
    eval_rollout_dir: eval_rollouts  # Directory where evaluation rollouts are stored
    train_episodes: *shared_train_episodes  # Episodes to run during training loops
    eval_episodes: *shared_eval_episodes  # Episodes to run during evaluation loops

experiments:  # Concrete experiment profiles selectable via run.py
  gaplock_ppo:  # PPO learner paired with a scripted defender
    env:
      <<: *env_base
      update: 10  # Number of environment steps between policy updates
    agents:
      roster:
        - <<: *attacker_base
          algo: ppo  # Learning algorithm assigned to the attacker
          config_ref: ppo  # Key mapping into the PPO hyperparameter block
        - *defender_policy
    reward:
      <<: *reward_base
    ppo:
      actor_lr: 0.0003  # Learning rate for the actor network
      critic_lr: 0.0005  # Learning rate for the critic network
      device: cpu  # Compute device used for PPO training
      gamma: 0.99  # Discount factor for future rewards
      lam: 0.95  # GAE lambda controlling advantage smoothing
      ent_coef: 0.015  # Initial entropy regularization weight
      ent_coef_schedule:
        start: 0.015  # Entropy coefficient at the beginning of training
        final: 0.005  # Entropy coefficient after decay completes
        decay_start: 500  # Episode index to begin decaying entropy weight
        decay_episodes: 1500  # Number of episodes over which to decay entropy
      clip_eps: 0.2  # PPO clipping parameter for policy ratio
      update_epochs: 10  # Gradient update passes per batch
      minibatch_size: 512  # Batch size for each PPO minibatch
      train_episodes: *shared_train_episodes  # Training episodes allocated to PPO
      eval_episodes: *shared_eval_episodes  # Evaluation episodes for validation
      save_dir: checkpoints  # Directory to store PPO checkpoints
      checkpoint_name: ppo_gaplock.pt  # Filename for the best PPO checkpoint
    main:
      <<: *main_base
      checkpoint: checkpoints/ppo_gaplock.pt  # Path to load/save PPO checkpoints
      wandb:
        <<: *wandb_base
        group: ppo  # W&B group label for PPO runs
        tags: [gaplock, ppo]  # W&B tags to categorize PPO runs

  gaplock_td3:  # TD3 learner with the same defender matchup
    env:
      <<: *env_base
      update: 1  # Frequency of TD3 environment updates (per step)
    agents:
      roster:
        - <<: *attacker_base
          algo: td3  # Use TD3 for the attacker network
          config_ref: td3  # Key for TD3 hyperparameters
        - *defender_policy
    reward:
      <<: *reward_base
    td3:
      actor_lr: 0.0003  # Learning rate for the TD3 actor
      critic_lr: 0.001  # Learning rate for the TD3 critics
      device: cpu  # Compute device used for TD3 training
      gamma: 0.99  # Discount factor applied to returns
      tau: 0.005  # Target network smoothing factor
      policy_noise: 0.3  # Noise added to target actions during updates
      noise_clip: 0.5  # Clamp applied to policy noise magnitude
      policy_delay: 2  # Frequency of policy updates relative to critic updates
      batch_size: 512  # Batch size sampled from the replay buffer
      buffer_size: 100000  # Capacity of the replay buffer
      warmup_steps: 10000  # Steps collected before training begins
      exploration_noise: 0.25  # Initial action noise scale for exploration
      exploration_noise_final: 0.05  # Final exploration noise after decay
      exploration_noise_decay_steps: 250000  # Steps over which exploration noise decays
      hidden_dims: [256, 256]  # Hidden layer sizes for actor and critic networks
      train_episodes: *shared_train_episodes  # Training episodes allocated to TD3
      eval_episodes: *shared_eval_episodes  # Evaluation episodes for TD3 checkpoints
      save_dir: checkpoints  # Directory for TD3 checkpoint files
      checkpoint_name: td3_gaplock.pt  # Filename for TD3 checkpoints
    main:
      <<: *main_base
      checkpoint: checkpoints/td3_gaplock.pt  # Path used to resume TD3 checkpoints
      wandb:
        <<: *wandb_base
        group: td3  # W&B group label for TD3 runs
        tags: [gaplock, td3]  # W&B tags for TD3 experiments

  gaplock_dqn:  # DQN learner using discrete throttle-steer pairs
    env:
      <<: *env_base
      update: 2  # Number of environment steps per DQN update cycle
    agents:
      roster:
        - <<: *attacker_base
          algo: dqn  # Assign a DQN controller to the attacker
          config_ref: dqn  # Key selecting the DQN hyperparameters
        - *defender_policy
    reward:
      <<: *reward_base
    dqn:
      device: cpu  # Compute device used for DQN training
      lr: 0.0004  # Learning rate for the Q-network optimizer
      gamma: 0.99  # Discount factor used by the DQN backup
      batch_size: 512  # Mini-batch size sampled from replay
      buffer_size: 100000  # Replay buffer capacity
      target_update_interval: 500  # Steps between target network updates
      epsilon_start: 1.0  # Initial epsilon value for ε-greedy policy
      epsilon_end: 0.1  # Minimum epsilon after decay
      epsilon_decay_rate: 0.9995  # Multiplicative epsilon decay factor per step
      hidden_dims: [256, 256]  # Hidden layer sizes for the Q-network
      prioritized_replay: true  # Enable prioritized experience replay
      per_alpha: 0.6  # Priority exponent controlling sampling bias
      per_beta_start: 0.4  # Initial importance-sampling exponent
      per_beta_increment: 0.00005  # Increment applied to β after each step
      per_min_priority: 0.001  # Minimum allowed priority to avoid zero sampling
      per_epsilon: 1.0e-6  # Small constant added to TD errors for stability
      action_set:  # Discrete steering/throttle actions available to the attacker
        - [-0.50, 2.0]  # Hard left, slow throttle
        - [-0.25, 2.0]  # Medium left, slow throttle
        - [0.0, 2.0]  # Straight, slow throttle
        - [0.25, 2.0]  # Medium right, slow throttle
        - [0.50, 2.0]  # Hard right, slow throttle
        - [-0.50, 4.0]  # Hard left, medium throttle
        - [-0.25, 4.0]  # Medium left, medium throttle
        - [0.0, 4.0]  # Straight, medium throttle
        - [0.25, 4.0]  # Medium right, medium throttle
        - [0.50, 4.0]  # Hard right, medium throttle
        - [-0.50, 6.0]  # Hard left, fast throttle
        - [-0.25, 6.0]  # Medium left, fast throttle
        - [0.0, 6.0]  # Straight, fast throttle
        - [0.25, 6.0]  # Medium right, fast throttle
        - [0.50, 6.0]  # Hard right, fast throttle
        - [-0.50, 8.0]  # Hard left, faster throttle
        - [-0.25, 8.0]  # Medium left, faster throttle
        - [0.0, 8.0]  # Straight, faster throttle
        - [0.25, 8.0]  # Medium right, faster throttle
        - [0.50, 8.0]  # Hard right, faster throttle
        - [-0.50, 10.0]  # Hard left, maximum throttle
        - [-0.25, 10.0]  # Medium left, maximum throttle
        - [0.0, 10.0]  # Straight, maximum throttle
        - [0.25, 10.0]  # Medium right, maximum throttle
        - [0.50, 10.0]  # Hard right, maximum throttle
      train_episodes: *shared_train_episodes  # Training episodes allocated to DQN
      eval_episodes: *shared_eval_episodes  # Evaluation episodes for DQN checkpoints
      save_dir: checkpoints  # Directory for DQN checkpoints
      checkpoint_name: dqn_gaplock.pt  # Filename for DQN checkpoints
    main:
      <<: *main_base
      checkpoint: checkpoints/dqn_gaplock.pt  # Path used to resume DQN checkpoints
      wandb:
        <<: *wandb_base
        group: dqn  # W&B group label for DQN runs
        tags: [gaplock, dqn]  # W&B tags for DQN experiments
