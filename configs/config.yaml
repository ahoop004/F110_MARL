env:
  seed: 42
  n_agents: 2
  max_steps: 5000
  timestep: 0.01
  integrator: 'RK4'
  render_interval: 100
  update: 1

  map_dir: "maps"
  map_yaml: "Shanghai_map.yaml"
  map: 'Shanghai_map.yaml'
  map_ext: .png
  render_mode: 'human'
  # TODO: include explicit map_image relative path once assets are standardized, enabling renderer cache priming.



  lidar_beams: 1080
  lidar_range: 30.0
  lidar_dist: 0.0
  start_thresh: 0.5
  vehicle_params:
    mu: 1.0489
    C_Sf: 4.718
    C_Sr: 5.4562
    lf: 0.15875
    lr: 0.17145
    h: 0.074
    m: 3.74
    I: 0.04712
    s_min: -0.4189
    s_max: 0.4189
    sv_min: -3.2
    sv_max: 3.2
    v_switch: 7.319
    a_max: 9.51
    v_min: -5.0
    v_max: 10.0
    width: 0.31
    length: 0.58
  
  start_poses:
    - [0.0, 0.0, 0.0]
    - [2.5, 0.5, 0.0]
  start_pose_options:
    -
      - [0.0, 0.1, 0.0]
      - [2.0, -0.1, 0.0]
    -
      - [0.0, -0.5, 0.0]
      - [2.5, 0.5, 0.0]
    -
      - [0.0, 0.5, 0.0]
      - [3.0, 0.2, 0.0]
    -
      - [0.1, 0.5, 0.0]
      - [3.5, 0.0, 0.0]
    -
      - [3.0, 0.0, 0.0]
      - [2.0, 0.5, 0.0]
    -
      - [0.0, -0.5, 0.0]
      - [2.5, 0.5, 0.0]
    -
      - [0.0, 0.5, 0.0]
      - [3.0, -0.2, 0.0]
    -
      - [0.0, -0.5, 0.0]
      - [3.5, 0.5, 0.0]
  start_pose_back_gap: 3.0
  start_pose_min_spacing: 3.0

mode: "train"

ppo:
  actor_lr: 0.0005
  critic_lr: 0.001
  gamma: 0.99
  lam: 0.95
  ent_coef: 0.01
  clip_eps: 0.2
  update_epochs: 10
  minibatch_size: 64
  train_episodes: 5000
  eval_episodes: 5
  save_dir: checkpoints/
  rolling_avg_window: 10

td3:
  actor_lr: 0.001
  critic_lr: 0.001
  gamma: 0.99
  tau: 0.005
  policy_noise: 0.2
  noise_clip: 0.5
  policy_delay: 2
  batch_size: 128
  buffer_size: 100000
  warmup_steps: 1000
  exploration_noise: 0.1
  hidden_dims: [256, 256]

dqn:
  gamma: 0.99
  lr: 0.0005
  batch_size: 64
  buffer_size: 50000
  target_update_interval: 500
  epsilon_start: 0.9
  epsilon_end: 0.05
  epsilon_decay: 20000
  hidden_dims: [256, 256]
  prioritized_replay: true
  per_alpha: 0.6
  per_beta_start: 0.4
  per_beta_increment: 0.0001
  per_min_priority: 0.001
  per_epsilon: 1.0e-6
  action_set:
    - [-0.4, 0.5]
    - [0.0, 0.5]
    - [0.4, 0.5]
    - [0.0, 1.0]
    - [0.0, 0.0]

reward:
  # mode: "pursuit"   # "basic", "pursuit", or "herding"
  alive_bonus: 0.02
  forward_scale: 0.15
  reverse_penalty: 1.5
  lateral_penalty: 0.05
  target_distance_scale: 0.1
  target_wall_bonus: 0.5
  self_wall_penalty: 0.3
  herd_bonus: 100.0
  ego_collision_penalty: -40.0
  spin_penalty: 2.0
  spin_thresh: 0.52   # radians ~30Â°
main:
  mode: train
  wandb:
    enabled: true
    project: marl-f110
    entity: null   # optional: set your wandb team/user
    tags:
      - dev
  tensorboard_dir: runs/
  save_eval_rollouts: false
  eval_rollout_dir: eval_rollouts/
  checkpoint: checkpoints/ppo_best.pt
