# Draft multi-agent experiment profiles (two attackers + one defender).
default_experiment: gaplock_multi_independent

shared:
  env: &env_base
    seed: 42
    n_agents: 3
    max_steps: 1000
    timestep: 0.01
    integrator: RK4
    render_interval: 0
    map_dir: maps
    map_yaml: Shanghai_map.yaml
    map: Shanghai_map.yaml
    map_ext: .png
    render_mode: human
    start_poses:
      - [0.0, 0.0, 0.0]      # Attacker A spawn
      - [1.5, 0.1, 0.0]      # Attacker B spawn
      - [3.0, -0.1, 0.0]     # Defender spawn
    start_pose_options:
      -
        - [0.0, 0.0, 0.0]
        - [1.5, 0.1, 0.0]
        - [3.0, -0.1, 0.0]
      -
        - [0.0, 0.0, 0.0]
        - [1.5, -0.1, 0.0]
        - [3.0, 0.1, 0.0]
    render_mode: human

  reward: &reward_base
    target_crash_reward: 100.0
    ego_collision_penalty: -2.0
    truncation_penalty: 0.0
    success_once: true
    reward_horizon: null
    reward_clip: null

  defender_policy: &defender_policy
    algo: follow_gap
    slot: 2
    agent_id: car_2
    role: defender
    params:
      max_speed: 18.0
      min_speed: 3.0
      steering_gain: 0.85
      bubble_radius: 3
      steer_smooth: 0.35

  attacker_template: &attacker_template
    wrappers:
      - factory: obs
        params:
          max_scan: 30.0
          normalize: true
          target_role: defender
    params: {}

  main: &main_base
    mode: train_eval
    save_eval_rollouts: true
    eval_rollout_dir: eval_rollouts
    train_episodes: 2000
    eval_episodes: 5
    output_root: outputs
    feature_flags:
      multi_agent_draft: true

experiments:
  gaplock_multi_independent:
    env:
      <<: *env_base
      update: 5
    agents:
      roster:
        - <<: *attacker_template
          slot: 0
          agent_id: car_0
          role: attacker_alpha
          algo: ppo
          config_ref: ppo_independent
        - <<: *attacker_template
          slot: 1
          agent_id: car_1
          role: attacker_bravo
          algo: ppo
          config_ref: ppo_independent
        - *defender_policy
    reward:
      <<: *reward_base
    ppo_independent:
      actor_lr: 0.0003
      critic_lr: 0.0005
      device: cuda
      gamma: 0.99
      lam: 0.95
      ent_coef: 0.015
      ent_coef_schedule:
        start: 0.015
        final: 0.005
        decay_start: 500
        decay_episodes: 1500
      clip_eps: 0.2
      update_epochs: 10
      minibatch_size: 512
      train_episodes: 2000
      eval_episodes: 5
      save_dir: checkpoints
      checkpoint_name: ppo_multi_best.pt
    main:
      <<: *main_base
      checkpoint: checkpoints/ppo_multi_best.pt
      wandb:
        enabled: true
        project: marl-f110
        entity: ahoop004-old-dominion-university
        group: ppo-multi-draft
        tags: [ppo, multi-agent]
